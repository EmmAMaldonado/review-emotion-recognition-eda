{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseteo del directorio principal\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\someo\\Downloads\\Review_EDA_Emotion_Recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_metadata = pd.read_csv('.\\data\\Tabla Normalizada - Metadata.csv')\n",
    "df_data_type = pd.read_csv('.\\data\\Tabla Normalizada - Data type.csv')\n",
    "df_participants = pd.read_csv('.\\data\\Tabla Normalizada - Participants.csv')\n",
    "df_self_report = pd.read_csv('.\\data\\Tabla Normalizada - Self report.csv')\n",
    "df_emotion_elicitation_techniques = pd.read_csv('.\\data\\Tabla Normalizada - Emotion elicitation techniques.csv')\n",
    "df_eda = pd.read_csv('.\\data\\Tabla Normalizada - EDA.csv')\n",
    "df_statistical_learning_models = pd.read_csv('.\\data\\Tabla Normalizada - Statistical Learning model.csv')\n",
    "df_performances = pd.read_csv('.\\data\\Tabla Normalizada - Performances.csv')\n",
    "df_alg_perf = pd.read_csv('.\\data\\Tabla Normalizada - Alg_Perf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data\n",
    "df_alg_perf = df_alg_perf[df_alg_perf['affective_model'] == 'dimensional']\n",
    "df_alg_perf = df_alg_perf[df_alg_perf['is_classifier'].isin(['x', 'X'])]\n",
    "df_alg_perf = df_alg_perf[df_alg_perf['class_model_output_categories'].isin(['HA, LA', 'HV, LV', 'LA, HA', 'LV, HV'])]\n",
    "df_alg_perf['class_model_output_categories'] = df_alg_perf['class_model_output_categories'].replace(['LA, HA', 'LV, HV'], ['HA, LA', 'HV, LV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate records in df_participants and df_performances\n",
    "df_participants_unique = df_participants[['paper_id', 'N']].drop_duplicates(subset=['paper_id'])\n",
    "df_performances_unique = df_performances[['paper_id', 'accuracy']].drop_duplicates(subset=['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data\n",
    "df_merged = df_alg_perf.merge(df_participants_unique, on='paper_id', how='left')\n",
    "df_merged = df_merged.merge(df_performances_unique, on='paper_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dataframe as an Excel file\n",
    "df_merged.to_excel('./data/df_merged.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_models = df_merged[df_merged['class_model_output_categories'] == 'HV, LV']\n",
    "arousal_models = df_merged[df_merged['class_model_output_categories'] == 'HA, LA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a temporary 'key' column to both valence and arousal models dataframes for the cross join\n",
    "valence_models = valence_models.assign(key=1)\n",
    "arousal_models = arousal_models.assign(key=1)\n",
    "\n",
    "# Merge valence and arousal models using a cross join within each paper\n",
    "merged_models = pd.merge(valence_models, arousal_models, on=['paper_id', 'key'])\n",
    "\n",
    "# Drop the temporary 'key' column\n",
    "merged_models = merged_models.drop(columns=['key'])\n",
    "\n",
    "# Keep only the relevant columns\n",
    "merged_models = merged_models[[\n",
    "    'paper_id', 'model_id_x', 'apa_citation_x', 'year_x', 'affective_model_x', 'is_classifier_x',\n",
    "    'class_model_output_number_x', 'class_model_output_categories_x', 'N_x',\n",
    "    'model_id_y', 'class_model_output_number_y', 'class_model_output_categories_y', 'N_y'\n",
    "]]\n",
    "\n",
    "# Rename columns\n",
    "merged_models.columns = [\n",
    "    'paper_id', 'model_id_valence', 'apa_citation', 'year', 'affective_model', 'is_classifier',\n",
    "    'class_model_output_number_valence', 'class_model_output_categories_valence', 'N_valence', 'accuracy_valence',\n",
    "    'model_id_arousal', 'class_model_output_number_arousal', 'class_model_output_categories_arousal', 'N_arousal', 'accuracy_arousal'\n",
    "]\n",
    "\n",
    "# Group by paper_id and model_id_valence, and generate comparison_model_id\n",
    "merged_models['comparison_model_id'] = merged_models.groupby(['paper_id', 'model_id_valence']).cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non-numeric values to NaN\n",
    "merged_models['accuracy_arousal'] = pd.to_numeric(merged_models['accuracy_arousal'], errors='coerce')\n",
    "merged_models['accuracy_valence'] = pd.to_numeric(merged_models['accuracy_valence'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with numeric values in both columns\n",
    "filtered_models = merged_models.dropna(subset=['accuracy_arousal', 'accuracy_valence'])\n",
    "\n",
    "filtered_models.to_excel('./data/df_merged_2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data types of the columns in the filtered_models dataframe\n",
    "print(filtered_models.dtypes)\n",
    "\n",
    "# Convert necessary columns to numeric types\n",
    "filtered_models['N_arousal'] = pd.to_numeric(filtered_models['N_arousal'], errors='coerce')\n",
    "filtered_models['N_valence'] = pd.to_numeric(filtered_models['N_valence'], errors='coerce')\n",
    "filtered_models['accuracy_arousal'] = pd.to_numeric(filtered_models['accuracy_arousal'], errors='coerce')\n",
    "filtered_models['accuracy_valence'] = pd.to_numeric(filtered_models['accuracy_valence'], errors='coerce')\n",
    "\n",
    "# Drop any rows with missing values\n",
    "filtered_models = filtered_models.dropna(subset=['N_arousal', 'N_valence', 'accuracy_arousal', 'accuracy_valence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by paper_id and calculate the standard deviation for arousal and valence\n",
    "grouped_sd = filtered_models.groupby('paper_id').agg({'accuracy_arousal': 'std', 'accuracy_valence': 'std'}).reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "grouped_sd.columns = ['paper_id', 'SD_arousal', 'SD_valence']\n",
    "\n",
    "# Merge the grouped_sd DataFrame with the filtered_models DataFrame on the paper_id column\n",
    "filtered_models = filtered_models.merge(grouped_sd, on='paper_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming 'filtered_models' is the DataFrame containing your data\n",
    "\n",
    "# Calculate the effect size for each model\n",
    "filtered_models['effect_size'] = filtered_models['accuracy_arousal'] - filtered_models['accuracy_valence']\n",
    "filtered_models['variance'] = (1 / filtered_models['N_arousal']) + (1 / filtered_models['N_valence'])\n",
    "\n",
    "# Group models by paper_id and calculate the mean effect size and variance for each paper\n",
    "grouped_models = filtered_models.groupby('paper_id').agg({'effect_size': 'mean', 'variance': 'mean'}).reset_index()\n",
    "\n",
    "# Calculate the weights\n",
    "grouped_models['weight'] = 1 / grouped_models['variance']\n",
    "\n",
    "# Calculate the weighted mean effect size\n",
    "weighted_mean_effect_size = np.sum(grouped_models['weight'] * grouped_models['effect_size']) / np.sum(grouped_models['weight'])\n",
    "\n",
    "# Calculate the variance of the weighted mean effect size\n",
    "variance_weighted_mean_effect_size = 1 / np.sum(grouped_models['weight'])\n",
    "\n",
    "# Calculate the standard error and confidence interval of the weighted mean effect size\n",
    "standard_error = np.sqrt(variance_weighted_mean_effect_size)\n",
    "critical_value = stats.norm.ppf(0.975)  # For a 95% confidence interval\n",
    "lower_bound = weighted_mean_effect_size - critical_value * standard_error\n",
    "upper_bound = weighted_mean_effect_size + critical_value * standard_error\n",
    "\n",
    "# Print the results\n",
    "print(\"Weighted mean effect size:\", weighted_mean_effect_size)\n",
    "print(\"95% Confidence interval:\", (lower_bound, upper_bound))\n",
    "\n",
    "# Add standard error, lower bound, and upper bound to the grouped_models DataFrame\n",
    "grouped_models['standard_error'] = np.sqrt(grouped_models['variance'])\n",
    "grouped_models['lower_bound'] = grouped_models['effect_size'] - critical_value * grouped_models['standard_error']\n",
    "grouped_models['upper_bound'] = grouped_models['effect_size'] + critical_value * grouped_models['standard_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot for individual study effect sizes and confidence intervals\n",
    "fig = go.Figure()\n",
    "\n",
    "for index, row in grouped_models.iterrows():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[row[\"lower_bound\"], row[\"upper_bound\"]],\n",
    "            y=[row[\"paper_id\"], row[\"paper_id\"]],\n",
    "            mode=\"lines\",\n",
    "            showlegend=False,\n",
    "            line=dict(color=\"blue\"),\n",
    "            name=\"Confidence Interval\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[row[\"effect_size\"]],\n",
    "            y=[row[\"paper_id\"]],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"blue\"),\n",
    "            showlegend=False,\n",
    "            name=\"Effect Size\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=weighted_mean_effect_size,\n",
    "    x1=weighted_mean_effect_size,\n",
    "    y0=-1,\n",
    "    y1=len(grouped_models),\n",
    "    yref=\"y\",\n",
    "    line=dict(color=\"red\"),\n",
    "    name=\"Overall Effect Size\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Forest Plot\",\n",
    "    xaxis_title=\"Effect Size\",\n",
    "    yaxis_title=\"Paper ID\",\n",
    "    yaxis=dict(autorange=\"reversed\"),\n",
    "    shapes=[dict(type='line', x0=0, x1=0, y0=-1, y1=len(grouped_models), yref=\"y\", line=dict(color=\"black\", dash='dash'), name=\"Line of No Effect\")],\n",
    "    legend=dict(\n",
    "        x=1,\n",
    "        y=1,\n",
    "        traceorder=\"normal\",\n",
    "        font=dict(family=\"sans-serif\", size=12),\n",
    "        bgcolor=\"LightSteelBlue\",\n",
    "        bordercolor=\"Black\",\n",
    "        borderwidth=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
